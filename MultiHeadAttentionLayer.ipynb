{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S4yXrSL6IkTx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from torch.utils.data.dataset import Subset\n",
        "\n",
        "import editdistance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextTransform:\n",
        "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
        "    def __init__(self):\n",
        "      self.char_map = { '': 0, ' ': 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27 }\n",
        "      self.index_map = { key: value for value, key in self.char_map.items() }\n",
        "\n",
        "    def text_to_int(self, text):\n",
        "      \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
        "      return [self.char_map.get(c, 0) for c in text]\n",
        "\n",
        "    def int_to_text(self, labels):\n",
        "      \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
        "      return ''.join([self.index_map[i] for i in labels])\n",
        "\n",
        "train_audio_transforms = nn.Sequential(\n",
        "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
        "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        ")\n",
        "\n",
        "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
        "\n",
        "text_transform = TextTransform()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnUErn3TIwIk",
        "outputId": "52a542e1-c77f-4cf9-bfc7-56afd33afefd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchaudio/functional/functional.py:576: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_processing(data, data_type=\"train\"):\n",
        "  spectrograms = []\n",
        "  labels = []\n",
        "  input_lengths = []\n",
        "  label_lengths = []\n",
        "  for (waveform, sample_rate, original_text, normalized_text, speaker_id, chapter_id, utterance_id) in data:\n",
        "    if data_type == 'train':\n",
        "      spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "    elif data_type == 'valid':\n",
        "      spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "    else:\n",
        "      raise Exception('data_type should be train or valid')\n",
        "    spectrograms.append(spec)\n",
        "    label = torch.Tensor(text_transform.text_to_int(normalized_text.lower()))\n",
        "    labels.append(label)\n",
        "    input_lengths.append(spec.shape[0]//2)\n",
        "    label_lengths.append(len(label))\n",
        "\n",
        "  spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "  labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "  return spectrograms, labels, input_lengths, label_lengths"
      ],
      "metadata": {
        "id": "ECYm9JvwJeOk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
        "  arg_maxes = torch.argmax(output, dim=2)\n",
        "  decodes = []\n",
        "  targets = []\n",
        "  for i, args in enumerate(arg_maxes):\n",
        "    decode = []\n",
        "    targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "    for j, index in enumerate(args):\n",
        "      if index != blank_label:\n",
        "        if collapse_repeated and j != 0 and index == args[j -1]:\n",
        "          continue\n",
        "        decode.append(index.item())\n",
        "    decodes.append(text_transform.int_to_text(decode))\n",
        "  return decodes, targets"
      ],
      "metadata": {
        "id": "9RagoTQjJvCE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BidirectionalLSTM(nn.Module):\n",
        "  def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "    super(BidirectionalLSTM, self).__init__()\n",
        "\n",
        "    self.BiLSTM = nn.LSTM(\n",
        "      input_size=rnn_dim, hidden_size=hidden_size,\n",
        "      num_layers=1, batch_first=batch_first, bidirectional=True\n",
        "    )\n",
        "    self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layer_norm(x)\n",
        "    x = F.gelu(x)\n",
        "    x, _ = self.BiLSTM(x)\n",
        "    x = self.dropout(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "pc-wGGFYJ0LC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 64),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, encoder_outputs):\n",
        "        # encoder_outputs : [batch_size, seq_len, hidden_dim]\n",
        "        energy = self.projection(encoder_outputs)  # [batch_size, seq_len, 1]\n",
        "        weights = F.softmax(energy.squeeze(-1), dim=1)  # [batch_size, seq_len]\n",
        "        # weights : [batch_size, seq_len]\n",
        "\n",
        "        outputs = (encoder_outputs * weights.unsqueeze(-1)).sum(dim=1)  # [batch_size, hidden_dim]\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "HAIU67iELilw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, d_model, dropout=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "        self.d_v = d_model // n_heads\n",
        "\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.out_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def attention(self, q, k, v, mask=None):\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        attention_weights = torch.softmax(scores, dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "        output = torch.matmul(attention_weights, v)\n",
        "        return output\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        q = self.q_linear(q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)\n",
        "        k = self.k_linear(k).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)\n",
        "        v = self.v_linear(v).view(batch_size, -1, self.n_heads, self.d_v).transpose(1,2)\n",
        "\n",
        "        attention_output = self.attention(q, k, v, mask=mask)\n",
        "        attention_output = attention_output.transpose(1,2).contiguous().view(batch_size, -1, self.n_heads*self.d_v)\n",
        "        output = self.out_linear(attention_output)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "z7rCn8vzPOPB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpeechRecognitionModel(nn.Module):\n",
        "  def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
        "    super(SpeechRecognitionModel, self).__init__()\n",
        "    n_feats = n_feats//2\n",
        "    self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
        "\n",
        "    # n ~residual~ cnn layers with filter size of 32\n",
        "    self.cnn_layers = nn.Sequential(*[\n",
        "      # ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
        "      nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=3//2)\n",
        "      for _ in range(n_cnn_layers * 2) # Times two to make this more compatible with the original model I am modifying, in which each one in n_cnn_layers created two Conv2d layers with skip connections\n",
        "    ])\n",
        "    self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
        "\n",
        "    self.birnn_layers = nn.Sequential(*[\n",
        "      # BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
        "      #                   hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
        "      BidirectionalLSTM(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
        "                        hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
        "      for i in range(n_rnn_layers)\n",
        "    ])\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "      nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
        "      nn.GELU(),\n",
        "      nn.Dropout(dropout),\n",
        "      nn.Linear(rnn_dim, n_class)\n",
        "    )\n",
        "\n",
        "    self.attention = MultiHeadAttention(n_heads = 16, d_model=rnn_dim, dropout=dropout)  # add self-attention layer\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # print('Initial x:', x.size())\n",
        "    x = self.cnn(x)\n",
        "    x = self.cnn_layers(x)\n",
        "    # print('After CNN layers:', x.size())\n",
        "    sizes = x.size()\n",
        "    x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "    x = x.transpose(1, 2) # (batch, time, feature)\n",
        "    x = self.fully_connected(x)\n",
        "    # print('After FC layer:', x.size())\n",
        "    x = self.attention(x,x,x) # (batch, time, feature)\n",
        "    # print('After attention:', x.size())\n",
        "    x = self.birnn_layers(x)\n",
        "    # print('After bilstm:', x.size())\n",
        "    x = self.classifier(x)\n",
        "    # print('After classifier:', x.size())\n",
        "    return x"
      ],
      "metadata": {
        "id": "dEplTayyLf1Q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch):\n",
        "  model.train()\n",
        "  data_len = len(train_loader.dataset)\n",
        "  for batch_idx, _data in enumerate(train_loader):\n",
        "    spectrograms, labels, input_lengths, label_lengths = _data \n",
        "    spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(spectrograms)  # (batch, time, n_class)\n",
        "    output = F.log_softmax(output, dim=2)\n",
        "    output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "    loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "    loss.backward()\n",
        "\n",
        "    # print('loss', loss.item())\n",
        "    # print('learning_rate', scheduler.get_lr())\n",
        "\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    if batch_idx % 10 == 0 or batch_idx == data_len:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(spectrograms), data_len,\n",
        "        100. * batch_idx / len(train_loader), loss.item())\n",
        "      )\n"
      ],
      "metadata": {
        "id": "tr0MIvmRKGDm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader, criterion):\n",
        "  print('\\nevaluating...')\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  test_char_edit_dist = []\n",
        "  test_word_edit_dist = []\n",
        "  with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "      spectrograms, labels, input_lengths, label_lengths = data \n",
        "      spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "      output = model(spectrograms) # (batch, time, n_class)\n",
        "      output = F.log_softmax(output, dim=2)\n",
        "      output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "      loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "      test_loss += loss.item() / len(test_loader)\n",
        "\n",
        "      decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "      print(\"Pred:\", decoded_preds)\n",
        "      print(\"Actual: \", decoded_targets)\n",
        "      for j in range(len(decoded_preds)):\n",
        "        test_char_edit_dist.append(editdistance.eval(decoded_targets[j], decoded_preds[j]))\n",
        "        test_word_edit_dist.append(editdistance.eval(decoded_targets[j].split(\" \"), decoded_preds[j].split(\" \")))\n",
        "\n",
        "  avg_char_edit_dist = sum(test_char_edit_dist)/len(test_char_edit_dist)\n",
        "  avg_word_edit_dist = sum(test_word_edit_dist)/len(test_word_edit_dist)\n",
        "\n",
        "  print(\"Test set:\")\n",
        "  print(\"Average loss: {:.4f}\".format(test_loss))\n",
        "  print(\"Average character edit distance: {:4f}\".format(avg_char_edit_dist))\n",
        "  print(\"Average word edit distance: {:.4f}\".format(avg_word_edit_dist))"
      ],
      "metadata": {
        "id": "SQviFuADKRwT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(learning_rate=5e-4, batch_size=20, epochs=10, train_url=\"train-clean-100\", test_url=\"test-clean\"):\n",
        "  hparams = {\n",
        "    \"n_cnn_layers\": 3,\n",
        "    \"n_rnn_layers\": 5,\n",
        "    \"rnn_dim\": 512,\n",
        "    \"n_class\": 29,\n",
        "    \"n_feats\": 128,\n",
        "    \"stride\": 2,\n",
        "    \"dropout\": 0.1,\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"epochs\": epochs\n",
        "  }\n",
        "\n",
        "  torch.manual_seed(7)\n",
        "\n",
        "  # Get ideal device (CPU, GPU, or MPS for Apple Silicon)\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "  # if torch.backends.mps.is_available():\n",
        "  #   if not torch.backends.mps.is_built():\n",
        "  #     torch.backends.mps.build()\n",
        "  #   device = torch.device(\"mps\")\n",
        "\n",
        "  if not os.path.isdir(\"./data\"):\n",
        "    os.makedirs(\"./data\")\n",
        "\n",
        "  train_dataset = torchaudio.datasets.LIBRITTS(root=\"data\", url=train_url, download=True)\n",
        "  test_dataset = torchaudio.datasets.LIBRITTS(root=\"data\", url=test_url, download=True)\n",
        "\n",
        "  indices = torch.randperm(len(train_dataset))[:10000]\n",
        "  train_dataset = Subset(train_dataset, indices)\n",
        "  indices = torch.randperm(len(test_dataset))[:1000]\n",
        "  test_dataset = Subset(test_dataset, indices)\n",
        "\n",
        "\n",
        "  kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "  train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                              batch_size=hparams['batch_size'],\n",
        "                              shuffle=True,\n",
        "                              collate_fn=lambda x: data_processing(x, 'train'),\n",
        "                              **kwargs)\n",
        "  test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                              batch_size=hparams['batch_size'],\n",
        "                              shuffle=False,\n",
        "                              collate_fn=lambda x: data_processing(x, 'valid'),\n",
        "                              **kwargs)\n",
        "\n",
        "  model = SpeechRecognitionModel(\n",
        "    hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
        "    hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
        "  ).to(device)\n",
        "\n",
        "  print(model)\n",
        "  print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
        "\n",
        "  optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
        "  criterion = nn.CTCLoss(blank=28).to(device)\n",
        "  scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
        "                                          steps_per_epoch=int(len(train_loader)),\n",
        "                                          epochs=hparams['epochs'],\n",
        "                                          anneal_strategy='linear')\n",
        "  \n",
        "  for epoch in range(1, epochs + 1):\n",
        "    train(model, device, train_loader, criterion, optimizer, scheduler, epoch)\n",
        "    test(model, device, test_loader, criterion)\n",
        "\n",
        "    torch.save({\n",
        "      \"epoch\": epoch,\n",
        "      \"model_state_dict\": model.state_dict(),\n",
        "      \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "    }, \"bilstmwattention.pt\")"
      ],
      "metadata": {
        "id": "z8XjxsAeKUNP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\""
      ],
      "metadata": {
        "id": "EypcrGMkKiFj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 5e-4\n",
        "batch_size = 10\n",
        "epochs = 1\n",
        "libri_train_set = \"train-clean-100\"\n",
        "libri_test_set = \"test-clean\"\n",
        "\n",
        "main(learning_rate, batch_size, epochs, libri_train_set, libri_test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oRcUQJc8KnL2",
        "outputId": "86b628ea-5929-49f7-fec8-8df527d1f7d5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7.19G/7.19G [08:35<00:00, 15.0MB/s]\n",
            "100%|██████████| 1.15G/1.15G [01:24<00:00, 14.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpeechRecognitionModel(\n",
            "  (cnn): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (cnn_layers): Sequential(\n",
            "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (fully_connected): Linear(in_features=2048, out_features=512, bias=True)\n",
            "  (birnn_layers): Sequential(\n",
            "    (0): BidirectionalLSTM(\n",
            "      (BiLSTM): LSTM(512, 512, batch_first=True, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (1): BidirectionalLSTM(\n",
            "      (BiLSTM): LSTM(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (2): BidirectionalLSTM(\n",
            "      (BiLSTM): LSTM(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (3): BidirectionalLSTM(\n",
            "      (BiLSTM): LSTM(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (4): BidirectionalLSTM(\n",
            "      (BiLSTM): LSTM(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (1): GELU(approximate='none')\n",
            "    (2): Dropout(p=0.1, inplace=False)\n",
            "    (3): Linear(in_features=512, out_features=29, bias=True)\n",
            "  )\n",
            "  (attention): MultiHeadAttention(\n",
            "    (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (out_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "  )\n",
            ")\n",
            "Num Model Parameters 32105501\n",
            "Train Epoch: 1 [0/10000 (0%)]\tLoss: 9.197830\n",
            "Train Epoch: 1 [100/10000 (1%)]\tLoss: 7.631120\n",
            "Train Epoch: 1 [200/10000 (2%)]\tLoss: 5.231533\n",
            "Train Epoch: 1 [300/10000 (3%)]\tLoss: 3.572065\n",
            "Train Epoch: 1 [400/10000 (4%)]\tLoss: 3.290235\n",
            "Train Epoch: 1 [500/10000 (5%)]\tLoss: 3.163473\n",
            "Train Epoch: 1 [600/10000 (6%)]\tLoss: 3.066609\n",
            "Train Epoch: 1 [700/10000 (7%)]\tLoss: 3.063911\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-ca2a49bc564d>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlibri_test_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test-clean\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibri_train_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibri_test_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-4c138309bd7d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(learning_rate, batch_size, epochs, train_url, test_url)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-3c3b085e84c0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, criterion, optimizer, scheduler, epoch)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# print('loss', loss.item())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.38 GiB (GPU 0; 14.75 GiB total capacity; 8.85 GiB already allocated; 1.52 GiB free; 12.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('bilstmwattention.pt')"
      ],
      "metadata": {
        "id": "XUdOmiheT1o7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = torchaudio.datasets.LIBRITTS(root=\"data\", url=\"test-clean\", download=True)"
      ],
      "metadata": {
        "id": "vYboHBwNf7H3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}