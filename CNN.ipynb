{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wave\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file 84_121123_000007_000001.wav with sample rate 24000 and length 24960\n",
      "Loaded file 84_121123_000008_000000.wav with sample rate 24000 and length 100800\n",
      "Loaded file 84_121123_000008_000001.wav with sample rate 24000 and length 132720\n",
      "Loaded file 84_121123_000008_000002.wav with sample rate 24000 and length 81120\n",
      "Loaded file 84_121123_000008_000003.wav with sample rate 24000 and length 396000\n",
      "Loaded file 84_121123_000008_000004.wav with sample rate 24000 and length 129120\n",
      "Loaded file 84_121123_000009_000000.wav with sample rate 24000 and length 87840\n",
      "Loaded file 84_121123_000009_000007.wav with sample rate 24000 and length 152880\n",
      "Loaded file 84_121123_000009_000008.wav with sample rate 24000 and length 33840\n",
      "Loaded file 84_121123_000010_000000.wav with sample rate 24000 and length 145440\n",
      "Loaded file test1.wav with sample rate 24000 and length 42481\n"
     ]
    }
   ],
   "source": [
    "# Set the path of the directory containing the WAV files\n",
    "directory_path = 'audio'\n",
    "\n",
    "# Initialize lists to store audio data and labels\n",
    "audio_data_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    # Check if the file is a WAV file\n",
    "    if filename.endswith('.wav'):\n",
    "        # Open the WAV file\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        audio_file = wave.open(file_path, 'rb')\n",
    "        \n",
    "        # Get the sample rate\n",
    "        sample_rate = audio_file.getframerate()\n",
    "        \n",
    "        # Read all frames from the file\n",
    "        audio_frames = audio_file.readframes(audio_file.getnframes())\n",
    "        \n",
    "        # Close the file\n",
    "        audio_file.close()\n",
    "        \n",
    "        # Convert the frames to a NumPy array\n",
    "        audio_data = np.frombuffer(audio_frames, dtype=np.int16)\n",
    "        \n",
    "        # Append the audio data and label to the lists\n",
    "        audio_data_list.append(audio_data)\n",
    "        labels_list.append(filename.split('_')[0])  # Assumes that the label is the first part of the filename\n",
    "        print(f'Loaded file {filename} with sample rate {sample_rate} and length {len(audio_data)}')\n",
    "# Convert the lists to NumPy arrays\n",
    "audio_data_array = np.array(audio_data_list, dtype=object)\n",
    "labels_array = np.array(labels_list)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(audio_data_array, labels_array, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "train_loader = DataLoader(dataset=train_data,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              )\n",
    "test_loader = DataLoader(dataset=test_data,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False,\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (conv_relu_stack): Sequential(\n",
      "    (0): Conv1d(8, 13, kernel_size=(3,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Conv1d(16, 11, kernel_size=(3,), stride=(1,))\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "    (8): Conv1d(32, 9, kernel_size=(3,), stride=(1,))\n",
      "    (9): ReLU()\n",
      "    (10): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Dropout(p=0.3, inplace=False)\n",
      "    (12): Conv1d(64, 7, kernel_size=(3,), stride=(1,))\n",
      "    (13): ReLU()\n",
      "    (14): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "    (15): Dropout(p=0.3, inplace=False)\n",
      "    (16): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Dropout(p=0.3, inplace=False)\n",
      "    (19): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (20): ReLU()\n",
      "    (21): Dropout(p=0.3, inplace=False)\n",
      "    (22): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (23): Softmax(dim=None)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\msako\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:88: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\cuda\\CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "# inputs = Input(shape=(8000,1))\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Using the nn.Sequential function, define a 5 layers of convolutions\n",
    "        self.conv_relu_stack =  nn.Sequential(\n",
    "            #First Conv1D layer\n",
    "            nn.Conv1d(8,13, 3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            #Second Conv1D layer\n",
    "            nn.Conv1d(16, 11, 3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            #Third Conv1D layer\n",
    "            nn.Conv1d(32, 9, 3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            #Fourth Conv1D layer\n",
    "            nn.Conv1d(64, 7, 3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            #Dense Layer 1\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            #Dense Layer 2\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            # inputs of linear are number of audio files used\n",
    "            nn.Linear(10,10),\n",
    "            nn.Softmax())\n",
    "\n",
    "        # model = Model(inputs, outputs)\n",
    "        # model.summary()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv_relu_stack(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.Linear(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross-entropy loss as the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# Define a pytorch optimizer using stochastic gradient descent (SGD)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def train(model, device, train_loader, criterion, optimizer, scheduler, epoch):\n",
    "# def train(model, device, train_loader, optimizer)\n",
    "#   model.train()\n",
    "#   data_len = len(train_loader.dataset)\n",
    "#   for batch_idx, _data in enumerate(train_loader):\n",
    "#     spectrograms, labels, input_lengths, label_lengths = _data \n",
    "#     spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     output = model(spectrograms)  # (batch, time, n_class)\n",
    "#     output = F.log_softmax(output, dim=2)\n",
    "#     output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "#     loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "#     loss.backward()\n",
    "\n",
    "#     # print('loss', loss.item())\n",
    "#     # print('learning_rate', scheduler.get_lr())\n",
    "\n",
    "#     optimizer.step()\n",
    "#     scheduler.step()\n",
    "#     if batch_idx % 10 == 0 or batch_idx == data_len:\n",
    "#       print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#         epoch, batch_idx * len(spectrograms), data_len,\n",
    "#         100. * batch_idx / len(train_loader), loss.item())\n",
    "#       )\n",
    "\n",
    "# def test(model, device, test_loader, criterion):\n",
    "#   print('\\nevaluating...')\n",
    "#   model.eval()\n",
    "#   test_loss = 0\n",
    "#   test_char_edit_dist = []\n",
    "#   test_word_edit_dist = []\n",
    "#   with torch.no_grad():\n",
    "#     for data in test_loader:\n",
    "#       spectrograms, labels, input_lengths, label_lengths = data \n",
    "#       spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "#       output = model(spectrograms) # (batch, time, n_class)\n",
    "#       output = F.log_softmax(output, dim=2)\n",
    "#       output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "#       loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "#       test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "#       decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "#       for j in range(len(decoded_preds)):\n",
    "#         test_char_edit_dist.append(editdistance.eval(decoded_targets[j], decoded_preds[j]))\n",
    "#         test_word_edit_dist.append(editdistance.eval(decoded_targets[j].split(\" \"), decoded_preds[j].split(\" \")))\n",
    "\n",
    "#   avg_char_edit_dist = sum(test_char_edit_dist)/len(test_char_edit_dist)\n",
    "#   avg_word_edit_dist = sum(test_word_edit_dist)/len(test_word_edit_dist)\n",
    "\n",
    "#   print(\"Test set:\")\n",
    "#   print(\"Average loss: {:.4f}\".format(test_loss))\n",
    "#   print(\"Average character edit distance: {:4f}\".format(avg_char_edit_dist))\n",
    "#   print(\"Average word edit distance: {:.4f}\".format(avg_word_edit_dist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    # set the model to train mode\n",
    "    model.train()\n",
    "    \n",
    "    losses = []\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        \n",
    "        # Compute training loss\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate model gradients from the loss and optimize the network\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss, current = loss.item(), batch * len(X)\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        losses.append(loss)\n",
    "        \n",
    "    return np.array(losses).mean()\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    # Set the model to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\t# no_grad mode doesn't compute gradients\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X) # compute predictions from X\n",
    "            test_loss +=  loss_fn(pred,y).item() # compute the test loss\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file 84_121123_000007_000001.wav with sample rate 24000 and length 24960\n",
      "Loaded file 84_121123_000008_000000.wav with sample rate 24000 and length 100800\n",
      "Loaded file 84_121123_000008_000001.wav with sample rate 24000 and length 132720\n",
      "Loaded file 84_121123_000008_000002.wav with sample rate 24000 and length 81120\n",
      "Loaded file 84_121123_000008_000003.wav with sample rate 24000 and length 396000\n",
      "Loaded file 84_121123_000008_000004.wav with sample rate 24000 and length 129120\n",
      "Loaded file 84_121123_000009_000000.wav with sample rate 24000 and length 87840\n",
      "Loaded file 84_121123_000009_000007.wav with sample rate 24000 and length 152880\n",
      "Loaded file 84_121123_000009_000008.wav with sample rate 24000 and length 33840\n",
      "Loaded file 84_121123_000010_000000.wav with sample rate 24000 and length 145440\n",
      "Loaded file test1.wav with sample rate 24000 and length 42481\n",
      "Data array shape: (11, 1)\n",
      "Label array shape: (11,)\n"
     ]
    }
   ],
   "source": [
    "# Set the path of the directory containing the WAV files\n",
    "directory_path = 'audio'\n",
    "\n",
    "# Initialize lists to store audio data and labels\n",
    "audio_data_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    # Check if the file is a WAV file\n",
    "    if filename.endswith('.wav'):\n",
    "        # Open the WAV file\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        audio_file = wave.open(file_path, 'rb')\n",
    "        \n",
    "        # Get the sample rate\n",
    "        sample_rate = audio_file.getframerate()\n",
    "        \n",
    "        # Read all frames from the file\n",
    "        audio_frames = audio_file.readframes(audio_file.getnframes())\n",
    "        \n",
    "        # Close the file\n",
    "        audio_file.close()\n",
    "        \n",
    "        # Convert the frames to a NumPy array\n",
    "        audio_data = np.frombuffer(audio_frames, dtype=np.int16)\n",
    "           # Append the audio data and label to the lists\n",
    "        audio_data_list.append(audio_data)\n",
    "        labels_list.append(filename.split('_')[0])  # Assumes that the label is the first part of the filename\n",
    "        print(f'Loaded file {filename} with sample rate {sample_rate} and length {len(audio_data)}')\n",
    "# Assuming audio files are all 1 second long and have the same sampling rate of 44100 Hz\n",
    "\n",
    "data_array = np.reshape(audio_data_array, (audio_data_array.shape[0], -1))  # Shape: (11, 44100)\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data_array, labels_array, test_size=0.2, random_state=42)\n",
    "print(\"Data array shape:\", data_array.shape)\n",
    "print(\"Label array shape:\", labels_array.shape)\n",
    "\n",
    "\n",
    "# clf = SVM(n_iters=1000)\n",
    "\n",
    "# clf.fit(train_data, train_labels)\n",
    "\n",
    "# # Predict labels for the test data\n",
    "# predictions = clf.predict(test_data)\n",
    "\n",
    "# # Compute the accuracy\n",
    "# accuracy = np.sum(predictions == test_labels) / len(test_labels)\n",
    "# print(f'SVM Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\msako\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:172: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:205.)\n",
      "  return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [33840] at entry 0 and [132720] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_48104\\3596242032.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'losses'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracies'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_48104\\4215426172.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\msako\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    626\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\msako\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\msako\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\msako\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    263\u001b[0m             \u001b[1;33m>>\u001b[0m\u001b[1;33m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Handle `CustomType` automatically\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \"\"\"\n\u001b[1;32m--> 265\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\msako\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\msako\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mcollate_numpy_array_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\msako\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\msako\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [33840] at entry 0 and [132720] at entry 1"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "# for plotting the training loss\n",
    "history = {'losses': [], 'accuracies': []}\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    history['losses'].append(train(train_loader, model, loss_fn, optimizer))\n",
    "    history['accuracies'].append(test(test_loader, model, loss_fn))\n",
    "    \n",
    "    plt.clf()\n",
    "    fig1 = plt.figure()\n",
    "    plt.plot(history['losses'], 'r-', lw=2, label='loss')\n",
    "    plt.legend()\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "    plt.clf()\n",
    "    fig2 = plt.figure()\n",
    "    plt.plot(history['accuracies'], 'b-', lw=1, label='accuracy')\n",
    "    plt.legend()\n",
    "#     display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
