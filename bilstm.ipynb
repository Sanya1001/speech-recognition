{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code was modified from [this source](https://colab.research.google.com/drive/1IPpwx4rX32rqHKpLz7dc8sOKspUa-YKO?undefined#scrollTo=XZodve8PGKfS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulljosh/Code/speech-recognition/.conda/lib/python3.10/site-packages/torchaudio/functional/functional.py:576: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import editdistance\n",
    "\n",
    "class TextTransform:\n",
    "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
    "    def __init__(self):\n",
    "      self.char_map = { '': 0, ' ': 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27 }\n",
    "      self.index_map = { key: value for value, key in self.char_map.items() }\n",
    "\n",
    "    def text_to_int(self, text):\n",
    "      \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
    "      return [self.char_map.get(c, 0) for c in text]\n",
    "\n",
    "    def int_to_text(self, labels):\n",
    "      \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
    "      return ''.join([self.index_map[i] for i in labels])\n",
    "\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
    ")\n",
    "\n",
    "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
    "\n",
    "text_transform = TextTransform()\n",
    "\n",
    "def data_processing(data, data_type=\"train\"):\n",
    "  spectrograms = []\n",
    "  labels = []\n",
    "  input_lengths = []\n",
    "  label_lengths = []\n",
    "  for (waveform, sample_rate, original_text, normalized_text, speaker_id, chapter_id, utterance_id) in data:\n",
    "    if data_type == 'train':\n",
    "      spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "    elif data_type == 'valid':\n",
    "      spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "    else:\n",
    "      raise Exception('data_type should be train or valid')\n",
    "    spectrograms.append(spec)\n",
    "    label = torch.Tensor(text_transform.text_to_int(normalized_text.lower()))\n",
    "    labels.append(label)\n",
    "    input_lengths.append(spec.shape[0]//2)\n",
    "    label_lengths.append(len(label))\n",
    "\n",
    "  spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "  labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "  return spectrograms, labels, input_lengths, label_lengths\n",
    "\n",
    "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
    "  arg_maxes = torch.argmax(output, dim=2)\n",
    "  decodes = []\n",
    "  targets = []\n",
    "  for i, args in enumerate(arg_maxes):\n",
    "    decode = []\n",
    "    targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
    "    for j, index in enumerate(args):\n",
    "      if index != blank_label:\n",
    "        if collapse_repeated and j != 0 and index == args[j -1]:\n",
    "          continue\n",
    "        decode.append(index.item())\n",
    "    decodes.append(text_transform.int_to_text(decode))\n",
    "  return decodes, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNNLayerNorm(nn.Module):\n",
    "#   \"\"\"Layer normalization built for cnns input\"\"\"\n",
    "#   def __init__(self, n_feats):\n",
    "#     super(CNNLayerNorm, self).__init__()\n",
    "#     self.layer_norm = nn.LayerNorm(n_feats)\n",
    "\n",
    "#   def forward(self, x):\n",
    "#     # x (batch, channel, feature, time)\n",
    "#     x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
    "#     x = self.layer_norm(x)\n",
    "#     return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
    "\n",
    "# class ResidualCNN(nn.Module):\n",
    "#   \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
    "#     except with layer norm instead of batch norm\n",
    "#   \"\"\"\n",
    "#   def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
    "#     super(ResidualCNN, self).__init__()\n",
    "\n",
    "#     self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "#     self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "#     self.dropout1 = nn.Dropout(dropout)\n",
    "#     self.dropout2 = nn.Dropout(dropout)\n",
    "#     self.layer_norm1 = CNNLayerNorm(n_feats)\n",
    "#     self.layer_norm2 = CNNLayerNorm(n_feats)\n",
    "\n",
    "#   def forward(self, x):\n",
    "#     residual = x  # (batch, channel, feature, time)\n",
    "#     x = self.layer_norm1(x)\n",
    "#     x = F.gelu(x)\n",
    "#     x = self.dropout1(x)\n",
    "#     x = self.cnn1(x)\n",
    "#     x = self.layer_norm2(x)\n",
    "#     x = F.gelu(x)\n",
    "#     x = self.dropout2(x)\n",
    "#     x = self.cnn2(x)\n",
    "#     x += residual\n",
    "#     return x # (batch, channel, feature, time)\n",
    "\n",
    "# class BidirectionalGRU(nn.Module):\n",
    "#   def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
    "#     super(BidirectionalGRU, self).__init__()\n",
    "\n",
    "#     self.BiGRU = nn.GRU(\n",
    "#       input_size=rnn_dim, hidden_size=hidden_size,\n",
    "#       num_layers=1, batch_first=batch_first, bidirectional=True\n",
    "#     )\n",
    "#     self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "#     self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#   def forward(self, x):\n",
    "#     x = self.layer_norm(x)\n",
    "#     x = F.gelu(x)\n",
    "#     x, _ = self.BiGRU(x)\n",
    "#     x = self.dropout(x)\n",
    "#     return x\n",
    "\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "  def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
    "    super(BidirectionalLSTM, self).__init__()\n",
    "\n",
    "    self.BiLSTM = nn.LSTM(\n",
    "      input_size=rnn_dim, hidden_size=hidden_size,\n",
    "      num_layers=1, batch_first=batch_first, bidirectional=True\n",
    "    )\n",
    "    self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.layer_norm(x)\n",
    "    x = F.gelu(x)\n",
    "    x, _ = self.BiLSTM(x)\n",
    "    x = self.dropout(x)\n",
    "    return x\n",
    "\n",
    "class SpeechRecognitionModel(nn.Module):\n",
    "  def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
    "    super(SpeechRecognitionModel, self).__init__()\n",
    "    n_feats = n_feats//2\n",
    "    self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
    "\n",
    "    # n ~residual~ cnn layers with filter size of 32\n",
    "    self.cnn_layers = nn.Sequential(*[\n",
    "      # ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
    "      nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=3//2)\n",
    "      for _ in range(n_cnn_layers * 2) # Times two to make this more compatible with the original model I am modifying, in which each one in n_cnn_layers created two Conv2d layers with skip connections\n",
    "    ])\n",
    "    self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
    "    self.birnn_layers = nn.Sequential(*[\n",
    "      # BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
    "      #                   hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
    "      BidirectionalLSTM(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
    "                        hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
    "      for i in range(n_rnn_layers)\n",
    "    ])\n",
    "    self.classifier = nn.Sequential(\n",
    "      nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
    "      nn.GELU(),\n",
    "      nn.Dropout(dropout),\n",
    "      nn.Linear(rnn_dim, n_class)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.cnn(x)\n",
    "    x = self.cnn_layers(x)\n",
    "    sizes = x.size()\n",
    "    x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
    "    x = x.transpose(1, 2) # (batch, time, feature)\n",
    "    x = self.fully_connected(x)\n",
    "    x = self.birnn_layers(x)\n",
    "    x = self.classifier(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch):\n",
    "  model.train()\n",
    "  data_len = len(train_loader.dataset)\n",
    "  for batch_idx, _data in enumerate(train_loader):\n",
    "    spectrograms, labels, input_lengths, label_lengths = _data \n",
    "    spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(spectrograms)  # (batch, time, n_class)\n",
    "    output = F.log_softmax(output, dim=2)\n",
    "    output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "    loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "    loss.backward()\n",
    "\n",
    "    # print('loss', loss.item())\n",
    "    # print('learning_rate', scheduler.get_lr())\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    if batch_idx % 10 == 0 or batch_idx == data_len:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(spectrograms), data_len,\n",
    "        100. * batch_idx / len(train_loader), loss.item())\n",
    "      )\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "  print('\\nevaluating...')\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  test_char_edit_dist = []\n",
    "  test_word_edit_dist = []\n",
    "  with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "      spectrograms, labels, input_lengths, label_lengths = data \n",
    "      spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "      output = model(spectrograms) # (batch, time, n_class)\n",
    "      output = F.log_softmax(output, dim=2)\n",
    "      output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "      loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "      test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "      decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "      for j in range(len(decoded_preds)):\n",
    "        test_char_edit_dist.append(editdistance.eval(decoded_targets[j], decoded_preds[j]))\n",
    "        test_word_edit_dist.append(editdistance.eval(decoded_targets[j].split(\" \"), decoded_preds[j].split(\" \")))\n",
    "\n",
    "  avg_char_edit_dist = sum(test_char_edit_dist)/len(test_char_edit_dist)\n",
    "  avg_word_edit_dist = sum(test_word_edit_dist)/len(test_word_edit_dist)\n",
    "\n",
    "  print(\"Test set:\")\n",
    "  print(\"Average loss: {:.4f}\".format(test_loss))\n",
    "  print(\"Average character edit distance: {:4f}\".format(avg_char_edit_dist))\n",
    "  print(\"Average word edit distance: {:.4f}\".format(avg_word_edit_dist))\n",
    "\n",
    "def main(learning_rate=5e-4, batch_size=20, epochs=10, train_url=\"train-clean-100\", test_url=\"test-clean\"):\n",
    "  hparams = {\n",
    "    \"n_cnn_layers\": 3,\n",
    "    \"n_rnn_layers\": 5,\n",
    "    \"rnn_dim\": 512,\n",
    "    \"n_class\": 29,\n",
    "    \"n_feats\": 128,\n",
    "    \"stride\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": epochs\n",
    "  }\n",
    "\n",
    "  torch.manual_seed(7)\n",
    "\n",
    "  # Get ideal device (CPU, GPU, or MPS for Apple Silicon)\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  # if torch.backends.mps.is_available():\n",
    "  #   if not torch.backends.mps.is_built():\n",
    "  #     torch.backends.mps.build()\n",
    "  #   device = torch.device(\"mps\")\n",
    "\n",
    "  if not os.path.isdir(\"./data\"):\n",
    "    os.makedirs(\"./data\")\n",
    "\n",
    "  train_dataset = torchaudio.datasets.LIBRITTS(root=\"data\", url=train_url, download=True)\n",
    "  test_dataset = torchaudio.datasets.LIBRITTS(root=\"data\", url=test_url, download=True)\n",
    "\n",
    "  kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "  train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                              batch_size=hparams['batch_size'],\n",
    "                              shuffle=True,\n",
    "                              collate_fn=lambda x: data_processing(x, 'train'),\n",
    "                              **kwargs)\n",
    "  test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                              batch_size=hparams['batch_size'],\n",
    "                              shuffle=False,\n",
    "                              collate_fn=lambda x: data_processing(x, 'valid'),\n",
    "                              **kwargs)\n",
    "\n",
    "  model = SpeechRecognitionModel(\n",
    "    hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "    hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
    "  ).to(device)\n",
    "\n",
    "  print(model)\n",
    "  print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
    "\n",
    "  optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
    "  criterion = nn.CTCLoss(blank=28).to(device)\n",
    "  scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
    "                                          steps_per_epoch=int(len(train_loader)),\n",
    "                                          epochs=hparams['epochs'],\n",
    "                                          anneal_strategy='linear')\n",
    "  \n",
    "  for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, criterion, optimizer, scheduler, epoch)\n",
    "    test(model, device, test_loader, criterion)\n",
    "\n",
    "    torch.save({\n",
    "      \"epoch\": epoch,\n",
    "      \"model_state_dict\": model.state_dict(),\n",
    "      \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \"blstm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Apple Silicon, fallback to CPU for operations that are not supported by MPS\n",
    "# (e.g. CTC loss)\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Conv2d.__init__() got an unexpected keyword argument 'dropout'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m libri_train_set \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain-clean-100\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m libri_test_set \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest-clean\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m main(learning_rate, batch_size, epochs, libri_train_set, libri_test_set)\n",
      "Cell \u001b[0;32mIn[13], line 101\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(learning_rate, batch_size, epochs, train_url, test_url)\u001b[0m\n\u001b[1;32m     90\u001b[0m train_loader \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mDataLoader(dataset\u001b[39m=\u001b[39mtrain_dataset,\n\u001b[1;32m     91\u001b[0m                             batch_size\u001b[39m=\u001b[39mhparams[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     92\u001b[0m                             shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     93\u001b[0m                             collate_fn\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: data_processing(x, \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     94\u001b[0m                             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     95\u001b[0m test_loader \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mDataLoader(dataset\u001b[39m=\u001b[39mtest_dataset,\n\u001b[1;32m     96\u001b[0m                             batch_size\u001b[39m=\u001b[39mhparams[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     97\u001b[0m                             shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     98\u001b[0m                             collate_fn\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: data_processing(x, \u001b[39m'\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     99\u001b[0m                             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 101\u001b[0m model \u001b[39m=\u001b[39m SpeechRecognitionModel(\n\u001b[1;32m    102\u001b[0m   hparams[\u001b[39m'\u001b[39;49m\u001b[39mn_cnn_layers\u001b[39;49m\u001b[39m'\u001b[39;49m], hparams[\u001b[39m'\u001b[39;49m\u001b[39mn_rnn_layers\u001b[39;49m\u001b[39m'\u001b[39;49m], hparams[\u001b[39m'\u001b[39;49m\u001b[39mrnn_dim\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    103\u001b[0m   hparams[\u001b[39m'\u001b[39;49m\u001b[39mn_class\u001b[39;49m\u001b[39m'\u001b[39;49m], hparams[\u001b[39m'\u001b[39;49m\u001b[39mn_feats\u001b[39;49m\u001b[39m'\u001b[39;49m], hparams[\u001b[39m'\u001b[39;49m\u001b[39mstride\u001b[39;49m\u001b[39m'\u001b[39;49m], hparams[\u001b[39m'\u001b[39;49m\u001b[39mdropout\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m    104\u001b[0m )\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    106\u001b[0m \u001b[39mprint\u001b[39m(model)\n\u001b[1;32m    107\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mNum Model Parameters\u001b[39m\u001b[39m'\u001b[39m, \u001b[39msum\u001b[39m([param\u001b[39m.\u001b[39mnelement() \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mparameters()]))\n",
      "Cell \u001b[0;32mIn[12], line 83\u001b[0m, in \u001b[0;36mSpeechRecognitionModel.__init__\u001b[0;34m(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride, dropout)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcnn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mConv2d(\u001b[39m1\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m3\u001b[39m, stride\u001b[39m=\u001b[39mstride, padding\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m)  \u001b[39m# cnn for extracting heirachal features\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[39m# n ~residual~ cnn layers with filter size of 32\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcnn_layers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\u001b[39m*\u001b[39m[\n\u001b[1;32m     84\u001b[0m   \u001b[39m# ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \u001b[39;00m\n\u001b[1;32m     85\u001b[0m   nn\u001b[39m.\u001b[39mConv2d(\u001b[39m32\u001b[39m, \u001b[39m32\u001b[39m, kernel_size\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, dropout\u001b[39m=\u001b[39mdropout, padding\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     86\u001b[0m   \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_cnn_layers \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m# Times two to make this more compatible with the original model I am modifying, in which each one in n_cnn_layers created two Conv2d layers with skip connections\u001b[39;00m\n\u001b[1;32m     87\u001b[0m ])\n\u001b[1;32m     88\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfully_connected \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(n_feats\u001b[39m*\u001b[39m\u001b[39m32\u001b[39m, rnn_dim)\n\u001b[1;32m     89\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbirnn_layers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\u001b[39m*\u001b[39m[\n\u001b[1;32m     90\u001b[0m   \u001b[39m# BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\u001b[39;00m\n\u001b[1;32m     91\u001b[0m   \u001b[39m#                   hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m   \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_rnn_layers)\n\u001b[1;32m     95\u001b[0m ])\n",
      "Cell \u001b[0;32mIn[12], line 85\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcnn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mConv2d(\u001b[39m1\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m3\u001b[39m, stride\u001b[39m=\u001b[39mstride, padding\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m)  \u001b[39m# cnn for extracting heirachal features\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[39m# n ~residual~ cnn layers with filter size of 32\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcnn_layers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\u001b[39m*\u001b[39m[\n\u001b[1;32m     84\u001b[0m   \u001b[39m# ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m   nn\u001b[39m.\u001b[39;49mConv2d(\u001b[39m32\u001b[39;49m, \u001b[39m32\u001b[39;49m, kernel_size\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, stride\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, dropout\u001b[39m=\u001b[39;49mdropout, padding\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     86\u001b[0m   \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_cnn_layers \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m# Times two to make this more compatible with the original model I am modifying, in which each one in n_cnn_layers created two Conv2d layers with skip connections\u001b[39;00m\n\u001b[1;32m     87\u001b[0m ])\n\u001b[1;32m     88\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfully_connected \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(n_feats\u001b[39m*\u001b[39m\u001b[39m32\u001b[39m, rnn_dim)\n\u001b[1;32m     89\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbirnn_layers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\u001b[39m*\u001b[39m[\n\u001b[1;32m     90\u001b[0m   \u001b[39m# BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\u001b[39;00m\n\u001b[1;32m     91\u001b[0m   \u001b[39m#                   hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m   \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_rnn_layers)\n\u001b[1;32m     95\u001b[0m ])\n",
      "\u001b[0;31mTypeError\u001b[0m: Conv2d.__init__() got an unexpected keyword argument 'dropout'"
     ]
    }
   ],
   "source": [
    "learning_rate = 5e-4\n",
    "batch_size = 10\n",
    "epochs = 10\n",
    "libri_train_set = \"train-clean-100\"\n",
    "libri_test_set = \"test-clean\"\n",
    "\n",
    "main(learning_rate, batch_size, epochs, libri_train_set, libri_test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
