{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code was modified from [this source](https://colab.research.google.com/drive/1IPpwx4rX32rqHKpLz7dc8sOKspUa-YKO?undefined#scrollTo=XZodve8PGKfS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import editdistance\n",
    "\n",
    "class TextTransform:\n",
    "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
    "    def __init__(self):\n",
    "      self.char_map = { '': 0, ' ': 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27 }\n",
    "      self.index_map = { key: value for value, key in self.char_map.items() }\n",
    "\n",
    "    def text_to_int(self, text):\n",
    "      \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
    "      return [self.char_map.get(c, 0) for c in text]\n",
    "\n",
    "    def int_to_text(self, labels):\n",
    "      \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
    "      return ''.join([self.index_map[i] for i in labels])\n",
    "\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
    ")\n",
    "\n",
    "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
    "\n",
    "text_transform = TextTransform()\n",
    "\n",
    "def data_processing(data, data_type=\"train\"):\n",
    "  spectrograms = []\n",
    "  labels = []\n",
    "  input_lengths = []\n",
    "  label_lengths = []\n",
    "  for (waveform, sample_rate, original_text, normalized_text, speaker_id, chapter_id, utterance_id) in data:\n",
    "    if data_type == 'train':\n",
    "      spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "    elif data_type == 'valid':\n",
    "      spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "    else:\n",
    "      raise Exception('data_type should be train or valid')\n",
    "    spectrograms.append(spec)\n",
    "    label = torch.Tensor(text_transform.text_to_int(normalized_text.lower()))\n",
    "    labels.append(label)\n",
    "    input_lengths.append(spec.shape[0]//2)\n",
    "    label_lengths.append(len(label))\n",
    "\n",
    "  spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "  labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "  return spectrograms, labels, input_lengths, label_lengths\n",
    "\n",
    "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
    "  arg_maxes = torch.argmax(output, dim=2)\n",
    "  decodes = []\n",
    "  targets = []\n",
    "  for i, args in enumerate(arg_maxes):\n",
    "    decode = []\n",
    "    targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
    "    for j, index in enumerate(args):\n",
    "      if index != blank_label:\n",
    "        if collapse_repeated and j != 0 and index == args[j -1]:\n",
    "          continue\n",
    "        decode.append(index.item())\n",
    "    decodes.append(text_transform.int_to_text(decode))\n",
    "  return decodes, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLayerNorm(nn.Module):\n",
    "  \"\"\"Layer normalization built for cnns input\"\"\"\n",
    "  def __init__(self, n_feats):\n",
    "    super(CNNLayerNorm, self).__init__()\n",
    "    self.layer_norm = nn.LayerNorm(n_feats)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x (batch, channel, feature, time)\n",
    "    x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
    "    x = self.layer_norm(x)\n",
    "    return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "  \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
    "    except with layer norm instead of batch norm\n",
    "  \"\"\"\n",
    "  def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
    "    super(ResidualCNN, self).__init__()\n",
    "\n",
    "    self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "    self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "    self.dropout1 = nn.Dropout(dropout)\n",
    "    self.dropout2 = nn.Dropout(dropout)\n",
    "    self.layer_norm1 = CNNLayerNorm(n_feats)\n",
    "    self.layer_norm2 = CNNLayerNorm(n_feats)\n",
    "\n",
    "  def forward(self, x):\n",
    "    residual = x  # (batch, channel, feature, time)\n",
    "    x = self.layer_norm1(x)\n",
    "    x = F.gelu(x)\n",
    "    x = self.dropout1(x)\n",
    "    x = self.cnn1(x)\n",
    "    x = self.layer_norm2(x)\n",
    "    x = F.gelu(x)\n",
    "    x = self.dropout2(x)\n",
    "    x = self.cnn2(x)\n",
    "    x += residual\n",
    "    return x # (batch, channel, feature, time)\n",
    "\n",
    "class BidirectionalGRU(nn.Module):\n",
    "  def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
    "    super(BidirectionalGRU, self).__init__()\n",
    "\n",
    "    self.BiGRU = nn.GRU(\n",
    "      input_size=rnn_dim, hidden_size=hidden_size,\n",
    "      num_layers=1, batch_first=batch_first, bidirectional=True\n",
    "    )\n",
    "    self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.layer_norm(x)\n",
    "    x = F.gelu(x)\n",
    "    x, _ = self.BiGRU(x)\n",
    "    x = self.dropout(x)\n",
    "    return x\n",
    "\n",
    "class SpeechRecognitionModel(nn.Module):\n",
    "  def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
    "    super(SpeechRecognitionModel, self).__init__()\n",
    "    n_feats = n_feats//2\n",
    "    self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
    "\n",
    "    # n residual cnn layers with filter size of 32\n",
    "    self.rescnn_layers = nn.Sequential(*[\n",
    "      ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
    "      for _ in range(n_cnn_layers)\n",
    "    ])\n",
    "    self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
    "    self.birnn_layers = nn.Sequential(*[\n",
    "      BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
    "                        hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
    "      for i in range(n_rnn_layers)\n",
    "    ])\n",
    "    self.classifier = nn.Sequential(\n",
    "      nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
    "      nn.GELU(),\n",
    "      nn.Dropout(dropout),\n",
    "      nn.Linear(rnn_dim, n_class)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.cnn(x)\n",
    "    x = self.rescnn_layers(x)\n",
    "    sizes = x.size()\n",
    "    x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
    "    x = x.transpose(1, 2) # (batch, time, feature)\n",
    "    x = self.fully_connected(x)\n",
    "    x = self.birnn_layers(x)\n",
    "    x = self.classifier(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch):\n",
    "  model.train()\n",
    "  data_len = len(train_loader.dataset)\n",
    "  for batch_idx, _data in enumerate(train_loader):\n",
    "    spectrograms, labels, input_lengths, label_lengths = _data \n",
    "    spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(spectrograms)  # (batch, time, n_class)\n",
    "    output = F.log_softmax(output, dim=2)\n",
    "    output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "    loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "    loss.backward()\n",
    "\n",
    "    # print('loss', loss.item())\n",
    "    # print('learning_rate', scheduler.get_lr())\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    if batch_idx % 10 == 0 or batch_idx == data_len:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(spectrograms), data_len,\n",
    "        100. * batch_idx / len(train_loader), loss.item())\n",
    "      )\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "  print('\\nevaluating...')\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  test_char_edit_dist = []\n",
    "  test_word_edit_dist = []\n",
    "  with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "      spectrograms, labels, input_lengths, label_lengths = data \n",
    "      spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "      output = model(spectrograms) # (batch, time, n_class)\n",
    "      output = F.log_softmax(output, dim=2)\n",
    "      output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "      loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "      test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "      decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "      for j in range(len(decoded_preds)):\n",
    "        test_char_edit_dist.append(editdistance.eval(decoded_targets[j], decoded_preds[j]))\n",
    "        test_word_edit_dist.append(editdistance.eval(decoded_targets[j].split(\" \"), decoded_preds[j].split(\" \")))\n",
    "\n",
    "  avg_char_edit_dist = sum(test_char_edit_dist)/len(test_char_edit_dist)\n",
    "  avg_word_edit_dist = sum(test_word_edit_dist)/len(test_word_edit_dist)\n",
    "\n",
    "  print(\"Test set:\")\n",
    "  print(\"Average loss: {:.4f}\".format(test_loss))\n",
    "  print(\"Average character edit distance: {:4f}\".format(avg_char_edit_dist))\n",
    "  print(\"Average word edit distance: {:.4f}\".format(avg_word_edit_dist))\n",
    "\n",
    "def main(learning_rate=5e-4, batch_size=20, epochs=10, train_url=\"train-clean-100\", test_url=\"test-clean\"):\n",
    "  hparams = {\n",
    "    \"n_cnn_layers\": 3,\n",
    "    \"n_rnn_layers\": 5,\n",
    "    \"rnn_dim\": 512,\n",
    "    \"n_class\": 29,\n",
    "    \"n_feats\": 128,\n",
    "    \"stride\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": epochs\n",
    "  }\n",
    "\n",
    "  torch.manual_seed(7)\n",
    "\n",
    "  # Get ideal device (CPU, GPU, or MPS for Apple Silicon)\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  # if torch.backends.mps.is_available():\n",
    "  #   if not torch.backends.mps.is_built():\n",
    "  #     torch.backends.mps.build()\n",
    "  #   device = torch.device(\"mps\")\n",
    "\n",
    "  if not os.path.isdir(\"./data\"):\n",
    "    os.makedirs(\"./data\")\n",
    "\n",
    "  train_dataset = torchaudio.datasets.LIBRITTS(root=\"data\", url=train_url, download=True)\n",
    "  test_dataset = torchaudio.datasets.LIBRITTS(root=\"data\", url=test_url, download=True)\n",
    "\n",
    "  kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "  train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                              batch_size=hparams['batch_size'],\n",
    "                              shuffle=True,\n",
    "                              collate_fn=lambda x: data_processing(x, 'train'),\n",
    "                              **kwargs)\n",
    "  test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                              batch_size=hparams['batch_size'],\n",
    "                              shuffle=False,\n",
    "                              collate_fn=lambda x: data_processing(x, 'valid'),\n",
    "                              **kwargs)\n",
    "\n",
    "  model = SpeechRecognitionModel(\n",
    "    hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "    hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
    "  ).to(device)\n",
    "\n",
    "  print(model)\n",
    "  print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
    "\n",
    "  optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
    "  criterion = nn.CTCLoss(blank=28).to(device)\n",
    "  scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
    "                                          steps_per_epoch=int(len(train_loader)),\n",
    "                                          epochs=hparams['epochs'],\n",
    "                                          anneal_strategy='linear')\n",
    "  \n",
    "  for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, criterion, optimizer, scheduler, epoch)\n",
    "    test(model, device, test_loader, criterion)\n",
    "\n",
    "    torch.save({\n",
    "      \"epoch\": epoch,\n",
    "      \"model_state_dict\": model.state_dict(),\n",
    "      \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \"blstm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Apple Silicon, fallback to CPU for operations that are not supported by MPS\n",
    "# (e.g. CTC loss)\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpeechRecognitionModel(\n",
      "  (cnn): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (rescnn_layers): Sequential(\n",
      "    (0): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (2): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fully_connected): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (birnn_layers): Sequential(\n",
      "    (0): BidirectionalGRU(\n",
      "      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=29, bias=True)\n",
      "  )\n",
      ")\n",
      "Num Model Parameters 23705373\n",
      "Train Epoch: 1 [0/33236 (0%)]\tLoss: 9.242327\n",
      "Train Epoch: 1 [100/33236 (0%)]\tLoss: 6.892880\n",
      "Train Epoch: 1 [200/33236 (1%)]\tLoss: 4.974225\n",
      "Train Epoch: 1 [300/33236 (1%)]\tLoss: 3.701903\n",
      "Train Epoch: 1 [400/33236 (1%)]\tLoss: 3.376833\n",
      "Train Epoch: 1 [500/33236 (2%)]\tLoss: 3.427119\n",
      "Train Epoch: 1 [600/33236 (2%)]\tLoss: 3.245562\n",
      "Train Epoch: 1 [700/33236 (2%)]\tLoss: 3.154782\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m libri_train_set \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain-clean-100\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m libri_test_set \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest-clean\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m main(learning_rate, batch_size, epochs, libri_train_set, libri_test_set)\n",
      "Cell \u001b[0;32mIn[15], line 117\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(learning_rate, batch_size, epochs, train_url, test_url)\u001b[0m\n\u001b[1;32m    111\u001b[0m scheduler \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mOneCycleLR(optimizer, max_lr\u001b[39m=\u001b[39mhparams[\u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[1;32m    112\u001b[0m                                         steps_per_epoch\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(train_loader)),\n\u001b[1;32m    113\u001b[0m                                         epochs\u001b[39m=\u001b[39mhparams[\u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    114\u001b[0m                                         anneal_strategy\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m--> 117\u001b[0m   train(model, device, train_loader, criterion, optimizer, scheduler, epoch)\n\u001b[1;32m    118\u001b[0m   test(model, device, test_loader, criterion)\n\u001b[1;32m    120\u001b[0m   torch\u001b[39m.\u001b[39msave({\n\u001b[1;32m    121\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m: epoch,\n\u001b[1;32m    122\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmodel_state_dict\u001b[39m\u001b[39m\"\u001b[39m: model\u001b[39m.\u001b[39mstate_dict(),\n\u001b[1;32m    123\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moptimizer_state_dict\u001b[39m\u001b[39m\"\u001b[39m: optimizer\u001b[39m.\u001b[39mstate_dict(),\n\u001b[1;32m    124\u001b[0m   }, \u001b[39m\"\u001b[39m\u001b[39mblstm.pt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, criterion, optimizer, scheduler, epoch)\u001b[0m\n\u001b[1;32m     12\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39m# (time, batch, n_class)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, labels, input_lengths, label_lengths)\n\u001b[0;32m---> 15\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     17\u001b[0m \u001b[39m# print('loss', loss.item())\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m# print('learning_rate', scheduler.get_lr())\u001b[39;00m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Code/speech-recognition/.conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/Code/speech-recognition/.conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 5e-4\n",
    "batch_size = 10\n",
    "epochs = 10\n",
    "libri_train_set = \"train-clean-100\"\n",
    "libri_test_set = \"test-clean\"\n",
    "\n",
    "main(learning_rate, batch_size, epochs, libri_train_set, libri_test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
